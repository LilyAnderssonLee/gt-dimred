{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import collections\n",
    "import gzip\n",
    "import numpy as np\n",
    "import os\n",
    "import phate\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import pickle as pk\n",
    "\n",
    "from collections import defaultdict\n",
    "import itertools\n",
    "\n",
    "# Interactive HTML tools\n",
    "from ipywidgets import interact\n",
    "import bokeh\n",
    "import bokeh.io\n",
    "from bokeh.io import push_notebook\n",
    "from bokeh.plotting import figure, show, save, output_notebook, output_file\n",
    "from bokeh.palettes import Category20b\n",
    "from bokeh.palettes import Category20\n",
    "from bokeh.palettes import Category10\n",
    "from bokeh.palettes import PRGn\n",
    "from bokeh.palettes import Set1\n",
    "\n",
    "# Machine-learning and dimensionality reduction tools\n",
    "import sklearn\n",
    "from sklearn import decomposition\n",
    "from sklearn.decomposition import PCA as PCA # We'll use this to check our implementation\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.manifold import MDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Note: PLINK command used to merged data\n",
    "# sort nhgri_coriell_affy_6_20140825_genotypes.bim > nhgri_coriell_affy_6_20140825_genotypes_sorted.bim\n",
    "# sort HRS_CLEAR.bim > HRS_CLEAR_sorted.bim\n",
    "\n",
    "# comm -12 nhgri_coriell_affy_6_20140825_genotypes_sorted.bim HRS_CLEAR_sorted.bim > common_snps_2.txt\n",
    "# plink --bfile merged_1000G_HRS --pca 200 --out merged_1000G_HRS_pca\n",
    "\n",
    "data_dir = '/Users/alex/Documents/Ethnicity/'\n",
    "\n",
    "# Define the files we'll be using\n",
    "pc_file = 'merged_1000G_HRS_pca.eigenvec'\n",
    "\n",
    "#aux_path = os.path.join(hrs_data_dir, aux_file)\n",
    "pc_path = os.path.join(data_dir, pc_file)\n",
    "\n",
    "# Import auxiliary data. Contains IDs and demographic information.\n",
    "# NOTE: The auxiliary data is sorted in an order different from the PC data.\n",
    "#aux_data = []\n",
    "#with open(aux_path) as input_file:\n",
    "#    for line in input_file:\n",
    "#        aux_data.append(line.strip().split(','))\n",
    "\n",
    "# Import PC data. This data must be converted to an array.\n",
    "with open(pc_path) as pc:\n",
    "    pca_contents = pc.readlines()\n",
    "\n",
    "pca_data = []\n",
    "\n",
    "for pc in pca_contents:\n",
    "    pca_data.append(pc.split()[2:len(pc)])\n",
    "\n",
    "pca_data_array = np.array(pca_data).astype(np.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15904, 200)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca_data_array.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing auxiliary data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For labelling we need to import auxiliary dataset from the 1000G and HRS datasets. The 1000G dataset is straightforward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import 1000G auxiliary data\n",
    "%store -r continents\n",
    "%store -r pop_by_continent\n",
    "%store -r pop\n",
    "%store -r indices_of_population_members\n",
    "%store -r name_by_code\n",
    "%store -r continent_by_population\n",
    "%store -r individuals\n",
    "%store -r population_by_individual\n",
    "%store -r individuals_by_population\n",
    "%store -r populations\n",
    "%store -r color_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The HRS data needs a bit more work as the variables aren't explicitly defined. We can import the dataset from the HRS notebook and then define values here as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import HRS auxiliary data\n",
    "%store -r aux_data_hrs\n",
    "%store -r hisp_dict\n",
    "%store -r hisp_dict_rev\n",
    "%store -r mex_dict\n",
    "%store -r mex_dict_rev\n",
    "%store -r race_dict\n",
    "%store -r race_dict_rev\n",
    "%store -r brn_dict\n",
    "%store -r brn_dict_rev\n",
    "%store -r racedb_dict\n",
    "%store -r racedb_dict_rev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#aux_data_hrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Labels for the HRS data aren't as clear - import the code to define those as well.\n",
    "# These auxiliary data sets will define how we label the observations\n",
    "\n",
    "# Columns are:\n",
    "# 0 = ID, 1 = Family ID, 2 = Birth Year\n",
    "# 3 = Hispanic, 4 = Detailed Hispanic, 5 = Race, 6 = Birth Region, 7 = Birth region name\n",
    "# 10= dbGaP race (Note: Black != AfrAm and White != Not_AfrAm)\n",
    "\n",
    "# Create multiple types of categorization based on variables to include\n",
    "aux_data_1 = [] # 1 - Birth region, race, Hispanic status, Mexican status\n",
    "aux_data_2 = [] # 2 - Race, Hispanic status, Mexican status\n",
    "aux_data_3 = [] # 3 - Birth region, race\n",
    "aux_data_4 = [] # 4 - Race, Hispanic status\n",
    "aux_data_5 = [] # 5 - Birth region\n",
    "aux_data_6 = [] # 6 - Birth region, Hispanic status, Mexican status\n",
    "aux_data_7 = [] # 7 - Birth region, Hispanic status\n",
    "\n",
    "individuals_hrs = []\n",
    "\n",
    "aux_data_dict_1 = defaultdict(list)\n",
    "aux_data_dict_2 = defaultdict(list)\n",
    "aux_data_dict_3 = defaultdict(list)\n",
    "aux_data_dict_4 = defaultdict(list)\n",
    "aux_data_dict_5 = defaultdict(list)\n",
    "aux_data_dict_6 = defaultdict(list)\n",
    "aux_data_dict_7 = defaultdict(list)\n",
    "\n",
    "# Get the lists (skip the first row as it's a header)\n",
    "for a in aux_data_hrs[1:]:\n",
    "#for a in aux_data_dict[subset][0:]:\n",
    "    individuals_hrs.append(a[0])\n",
    "    \n",
    "    temp_element = [a[0], '_'.join([brn_dict[a[7]], race_dict[a[5]], hisp_dict[a[3]], mex_dict[a[4]]])]    \n",
    "    aux_data_1.append(temp_element)\n",
    "    \n",
    "    temp_element = [a[0], '_'.join([race_dict[a[5]], hisp_dict[a[3]], mex_dict[a[4]]])]\n",
    "    aux_data_2.append(temp_element)\n",
    "    \n",
    "    temp_element = [a[0], '_'.join([brn_dict[a[7]], race_dict[a[5]]])]\n",
    "    aux_data_3.append(temp_element)\n",
    "    \n",
    "    temp_element = [a[0], '_'.join([race_dict[a[5]], hisp_dict[a[3]]])]\n",
    "    aux_data_4.append(temp_element)\n",
    "    \n",
    "    temp_element = [a[0], '_'.join([brn_dict[a[7]]])]\n",
    "    aux_data_5.append(temp_element)\n",
    "    \n",
    "    temp_element = [a[0], '_'.join([brn_dict[a[7]], hisp_dict[a[3]], mex_dict[a[4]]])]\n",
    "    aux_data_6.append(temp_element)\n",
    "    \n",
    "    temp_element = [a[0], '_'.join([brn_dict[a[7]], hisp_dict[a[3]]])]\n",
    "    aux_data_7.append(temp_element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "aux_to_use = aux_data_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We must define the population dictionary we wish to use\n",
    "# The following gives us a collection of all categories of some population and/or proxy for ethnicity:\n",
    "eth_proxy_set = set([a[1] for a in aux_to_use])\n",
    "pop_dict = dict()\n",
    "\n",
    "for e in eth_proxy_set:\n",
    "    el = e.split('_')\n",
    "    \n",
    "    if aux_to_use == aux_data_1:    \n",
    "        temp_brn = brn_dict_rev[el[0]]\n",
    "        temp_race = race_dict_rev[el[1]]\n",
    "        temp_hisp = hisp_dict_rev[el[2]]\n",
    "        temp_mex = mex_dict_rev[el[3]]\n",
    "        \n",
    "        pop_dict.update({e:temp_brn + ' ' + temp_race + ' ' + temp_hisp + ' ' + temp_mex})\n",
    "    elif aux_to_use == aux_data_2:\n",
    "        temp_race = race_dict_rev[el[0]]\n",
    "        temp_hisp = hisp_dict_rev[el[1]]\n",
    "        temp_mex = mex_dict_rev[el[2]]\n",
    "        temp_list = [temp_race, temp_hisp, temp_mex]\n",
    "        \n",
    "        pop_dict.update({e:temp_race + ' ' + temp_hisp + ' ' + temp_mex})\n",
    "    elif aux_to_use == aux_data_3:\n",
    "        temp_brn = brn_dict_rev[el[0]]\n",
    "        temp_race = race_dict_rev[el[1]]\n",
    "        \n",
    "        pop_dict.update({e: temp_brn + ' ' + temp_race})\n",
    "    elif aux_to_use == aux_data_4:\n",
    "        temp_race = race_dict_rev[el[0]]\n",
    "        temp_hisp = hisp_dict_rev[el[1]]\n",
    "        \n",
    "        pop_dict.update({e: temp_race + ' ' + temp_hisp})\n",
    "    elif aux_to_use == aux_data_5:\n",
    "        temp_brn = brn_dict_rev[el[0]]\n",
    "        \n",
    "        pop_dict.update({e: temp_brn})\n",
    "    elif aux_to_use == aux_data_6:\n",
    "        temp_brn = brn_dict_rev[el[0]]\n",
    "        temp_hisp = hisp_dict_rev[el[1]]\n",
    "        temp_mex = mex_dict_rev[el[2]]\n",
    "        \n",
    "        pop_dict.update({e: temp_brn + ' ' + temp_hisp + ' ' + temp_mex})\n",
    "    elif aux_to_use == aux_data_7:\n",
    "        temp_brn = brn_dict_rev[el[0]]\n",
    "        temp_hisp = hisp_dict_rev[el[1]]\n",
    "        \n",
    "        pop_dict.update({e: temp_brn + ' ' + temp_hisp})\n",
    "    \n",
    "    #print(el, temp_list)\n",
    "    #print(e.split('_'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set up an index of each population member and vice versa\n",
    "# We want to quickly access a given individual's population and a given population's individuals\n",
    "population_by_individual_hrs = defaultdict(int)\n",
    "individuals_by_population_hrs = defaultdict(list)\n",
    "\n",
    "for a in aux_to_use:\n",
    "    population_by_individual_hrs[a[0]] = a[1]\n",
    "    individuals_by_population_hrs[a[1]].append(a[0])\n",
    "    \n",
    "indices_of_population_members_hrs = defaultdict(list)\n",
    "\n",
    "for index, indiv in enumerate(individuals_hrs):\n",
    "    try:\n",
    "        indices_of_population_members_hrs[population_by_individual_hrs[indiv]].append(index)\n",
    "    except KeyError:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating projections\n",
    "\n",
    "Project various PCs to 2d via tSNE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#pc_list = list(range(2,20)) + list(range(20,51,10))\n",
    "#plex_list = [2, 25, 75]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#for pc in pc_list:\n",
    "#    for plex in plex_list:\n",
    "#        temp_proj = TSNE(n_components=2, perplexity=plex).fit_transform(pca_data_array[:,0:pc])\n",
    "#        print('Finished projecting for {PC, PLEX}: {' + str(pc) + ', ' + str(plex) + '}')\n",
    "#        np.savetxt('HRS_1000G_TSNE_PC_' + str(pc) + '_PLEX_' + str(plex), temp_proj)\n",
    "#        print('Finished saving for {PC, PLEX}: {' + str(pc) + ', ' + str(plex) + '}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MDS' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-40f0e0b6d866>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Try an MDS projection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmds_proj\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mMDS\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpca_data_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'MDS' is not defined"
     ]
    }
   ],
   "source": [
    "# Try an MDS projection\n",
    "#mds_proj= MDS(n_components=2).fit_transform(pca_data_array)[:, 0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting projections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Need to define colour dictionaries - just go with what was previously set up\n",
    "color_dict_1000g = {}\n",
    "for i, cont in enumerate(continents): \n",
    "    for j, pop in enumerate(pop_by_continent[cont]):\n",
    "        color_dict_1000g[pop] = Category20b[20][4*i+j%4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'B_H_O',\n",
       " 'B_N_N',\n",
       " 'O_H_M',\n",
       " 'O_H_O',\n",
       " 'O_N_N',\n",
       " 'W_H_M',\n",
       " 'W_H_O',\n",
       " 'W_H_U',\n",
       " 'W_N_N'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eth_proxy_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#color_dict_hrs = {}\n",
    "\n",
    "%store -r color_dict_born\n",
    "%store -r color_dict_race_hisp\n",
    "%store -r color_dict_race_hisp_mex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "color_dict_hrs = color_dict_race_hisp_mex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished created interactive HTML file on 10 PCs with perplexity 2\n",
      "Finished created interactive HTML file on 10 PCs with perplexity 25\n",
      "Finished created interactive HTML file on 10 PCs with perplexity 75\n",
      "Finished created interactive HTML file on 11 PCs with perplexity 2\n",
      "Finished created interactive HTML file on 11 PCs with perplexity 25\n",
      "Finished created interactive HTML file on 11 PCs with perplexity 75\n",
      "Finished created interactive HTML file on 12 PCs with perplexity 2\n",
      "Finished created interactive HTML file on 12 PCs with perplexity 25\n",
      "Finished created interactive HTML file on 12 PCs with perplexity 75\n",
      "Finished created interactive HTML file on 13 PCs with perplexity 2\n",
      "Finished created interactive HTML file on 13 PCs with perplexity 25\n",
      "Finished created interactive HTML file on 13 PCs with perplexity 75\n",
      "Finished created interactive HTML file on 14 PCs with perplexity 2\n",
      "Finished created interactive HTML file on 14 PCs with perplexity 25\n",
      "Finished created interactive HTML file on 14 PCs with perplexity 75\n",
      "Finished created interactive HTML file on 15 PCs with perplexity 2\n",
      "Finished created interactive HTML file on 15 PCs with perplexity 25\n",
      "Finished created interactive HTML file on 15 PCs with perplexity 75\n",
      "Finished created interactive HTML file on 16 PCs with perplexity 2\n",
      "Finished created interactive HTML file on 16 PCs with perplexity 25\n",
      "Finished created interactive HTML file on 16 PCs with perplexity 75\n",
      "Finished created interactive HTML file on 17 PCs with perplexity 2\n",
      "Finished created interactive HTML file on 17 PCs with perplexity 25\n",
      "Finished created interactive HTML file on 17 PCs with perplexity 75\n",
      "Finished created interactive HTML file on 18 PCs with perplexity 2\n",
      "Finished created interactive HTML file on 18 PCs with perplexity 25\n",
      "Finished created interactive HTML file on 18 PCs with perplexity 75\n",
      "Finished created interactive HTML file on 19 PCs with perplexity 2\n",
      "Finished created interactive HTML file on 19 PCs with perplexity 25\n",
      "Finished created interactive HTML file on 19 PCs with perplexity 75\n",
      "Finished created interactive HTML file on 20 PCs with perplexity 2\n",
      "Finished created interactive HTML file on 20 PCs with perplexity 25\n",
      "Finished created interactive HTML file on 20 PCs with perplexity 75\n",
      "Finished created interactive HTML file on 2 PCs with perplexity 2\n",
      "Finished created interactive HTML file on 2 PCs with perplexity 25\n",
      "Finished created interactive HTML file on 2 PCs with perplexity 75\n",
      "Finished created interactive HTML file on 30 PCs with perplexity 2\n",
      "Finished created interactive HTML file on 30 PCs with perplexity 25\n",
      "Finished created interactive HTML file on 30 PCs with perplexity 75\n",
      "Finished created interactive HTML file on 3 PCs with perplexity 2\n",
      "Finished created interactive HTML file on 3 PCs with perplexity 25\n",
      "Finished created interactive HTML file on 3 PCs with perplexity 75\n",
      "Finished created interactive HTML file on 40 PCs with perplexity 2\n",
      "Finished created interactive HTML file on 40 PCs with perplexity 25\n",
      "Finished created interactive HTML file on 40 PCs with perplexity 75\n",
      "Finished created interactive HTML file on 4 PCs with perplexity 2\n",
      "Finished created interactive HTML file on 4 PCs with perplexity 25\n",
      "Finished created interactive HTML file on 4 PCs with perplexity 75\n",
      "Finished created interactive HTML file on 50 PCs with perplexity 2\n",
      "Finished created interactive HTML file on 50 PCs with perplexity 25\n",
      "Finished created interactive HTML file on 50 PCs with perplexity 75\n",
      "Finished created interactive HTML file on 5 PCs with perplexity 2\n",
      "Finished created interactive HTML file on 5 PCs with perplexity 25\n",
      "Finished created interactive HTML file on 5 PCs with perplexity 75\n",
      "Finished created interactive HTML file on 6 PCs with perplexity 2\n",
      "Finished created interactive HTML file on 6 PCs with perplexity 25\n",
      "Finished created interactive HTML file on 6 PCs with perplexity 75\n",
      "Finished created interactive HTML file on 7 PCs with perplexity 2\n",
      "Finished created interactive HTML file on 7 PCs with perplexity 25\n",
      "Finished created interactive HTML file on 7 PCs with perplexity 75\n",
      "Finished created interactive HTML file on 8 PCs with perplexity 2\n",
      "Finished created interactive HTML file on 8 PCs with perplexity 25\n",
      "Finished created interactive HTML file on 8 PCs with perplexity 75\n",
      "Finished created interactive HTML file on 9 PCs with perplexity 2\n",
      "Finished created interactive HTML file on 9 PCs with perplexity 25\n",
      "Finished created interactive HTML file on 9 PCs with perplexity 75\n"
     ]
    }
   ],
   "source": [
    "# Directory where projection coordinates are stored\n",
    "proj_dir = '/Users/alex/Documents/Ethnicity/HRS_1000G_projections'\n",
    "\n",
    "# Labels for HRS\n",
    "if aux_to_use == aux_data_1:\n",
    "    aux_label = 'BORN_RACE_HISP_MEX'\n",
    "elif aux_to_use == aux_data_2:\n",
    "    aux_label = 'RACE_HISP_MEX'\n",
    "elif aux_to_use == aux_data_3:\n",
    "    aux_label = 'BORN_RACE'\n",
    "elif aux_to_use == aux_data_4:\n",
    "    aux_label = 'RACE_HISP'\n",
    "elif aux_to_use == aux_data_5:\n",
    "    aux_label = 'BORN'\n",
    "elif aux_to_use == aux_data_6:\n",
    "    aux_label = 'BORN_HISP_MEX'\n",
    "elif aux_to_use == aux_data_7:\n",
    "    aux_label = 'BORN_HISP'\n",
    "\n",
    "# Loop though each tSNE projection\n",
    "for file in os.listdir(proj_dir):\n",
    "    num_pcs = file.split('PC_')[1].split('_')[0]\n",
    "    plex = file.split('PLEX_')[1]\n",
    "    \n",
    "    ptitle_str = 'HRS+1000G tSNE on ' + str(num_pcs) + ' PCs, perplexity ' + str(plex)\n",
    "    fname_str = 'TSNE_HRS_1000G_' + aux_label + '_PC_' + str(num_pcs) + '_PLEX_' + str(plex)\n",
    "    ftitle_str = 'tSNE on HRS+1000G data, ' + str(num_pcs) + ' PCs, perplexity ' + str(plex)\n",
    "    arr_str = os.path.join(proj_dir,'HRS_1000G_TSNE_PC_' + str(num_pcs) + '_PLEX_' + str(plex))\n",
    "    \n",
    "    int_html_hrs_1000g(ptitle_str, fname_str, ftitle_str, arr_str)\n",
    "    print('Finished created interactive HTML file on ' + str(num_pcs) + ' PCs with perplexity ' + str(plex))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a custom function that creates interactive HTML based on file input\n",
    "# Inputs: Plot title, file name, file title, num_pcs, perplexity, array file location\n",
    "def int_html_hrs_1000g(ptitle, fname, ftitle, arr):\n",
    "    component_1_id = 0\n",
    "    component_2_id = 1\n",
    "\n",
    "    proj_data = np.loadtxt(arr)\n",
    "\n",
    "    proj_data_hrs = proj_data[0:(len(aux_data_hrs)-1),:]\n",
    "    proj_data_1000g = proj_data[len(aux_data_hrs)-1:,:]\n",
    "    \n",
    "    p = figure(plot_width=1500, plot_height=1200)\n",
    "    p.title.text = ptitle\n",
    "\n",
    "    # First loop is for HRS data\n",
    "    for pop in sorted(eth_proxy_set):\n",
    "        proj_pop = proj_data_hrs[indices_of_population_members_hrs[pop]]\n",
    "        p.circle(proj_pop[:,component_1_id], proj_pop[:,component_2_id], legend=pop_dict[pop],\n",
    "                 color = color_dict_hrs[pop])\n",
    "\n",
    "    # Second loop is for 1000G data\n",
    "    for cont in continents: \n",
    "        for pop in pop_by_continent[cont]:\n",
    "            proj_pop = proj_data_1000g[indices_of_population_members[pop]]\n",
    "            p.circle(proj_pop[:,component_1_id], proj_pop[:,component_2_id], \n",
    "                     legend=name_by_code[pop], color = color_dict[pop])\n",
    "\n",
    "    p.legend.location = \"top_left\"\n",
    "\n",
    "    p.legend.click_policy=\"hide\"\n",
    "\n",
    "    output_file(fname + '.html',title=ftitle)\n",
    "\n",
    "    save(p)\n",
    "\n",
    "def int_html_hrs_1000g_local(ptitle, fname, ftitle, arr):\n",
    "    component_1_id = 0\n",
    "    component_2_id = 1\n",
    "\n",
    "    proj_data = arr\n",
    "\n",
    "    proj_data_hrs = proj_data[0:(len(aux_data_hrs)-1),:]\n",
    "    proj_data_1000g = proj_data[len(aux_data_hrs)-1:,:]\n",
    "    \n",
    "    p = figure(plot_width=1500, plot_height=1200)\n",
    "    p.title.text = ptitle\n",
    "\n",
    "    # First loop is for HRS data\n",
    "    for pop in sorted(eth_proxy_set):\n",
    "        proj_pop = proj_data_hrs[indices_of_population_members_hrs[pop]]\n",
    "        p.circle(proj_pop[:,component_1_id], proj_pop[:,component_2_id], legend=pop_dict[pop],\n",
    "                 color = color_dict_hrs[pop])\n",
    "\n",
    "    # Second loop is for 1000G data\n",
    "    for cont in continents: \n",
    "        for pop in pop_by_continent[cont]:\n",
    "            proj_pop = proj_data_1000g[indices_of_population_members[pop]]\n",
    "            p.circle(proj_pop[:,component_1_id], proj_pop[:,component_2_id], \n",
    "                     legend=name_by_code[pop], color = color_dict[pop])\n",
    "\n",
    "    p.legend.location = \"top_left\"\n",
    "\n",
    "    p.legend.click_policy=\"hide\"\n",
    "\n",
    "    output_file(fname + '.html',title=ftitle)\n",
    "\n",
    "    save(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "int_html_hrs_1000g_local('test','test','test',mds_proj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create HTML of UMAP projections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Labels for HRS\n",
    "if aux_to_use == aux_data_1:\n",
    "    aux_label = 'BORN_RACE_HISP_MEX'\n",
    "elif aux_to_use == aux_data_2:\n",
    "    aux_label = 'RACE_HISP_MEX'\n",
    "elif aux_to_use == aux_data_3:\n",
    "    aux_label = 'BORN_RACE'\n",
    "elif aux_to_use == aux_data_4:\n",
    "    aux_label = 'RACE_HISP'\n",
    "elif aux_to_use == aux_data_5:\n",
    "    aux_label = 'BORN'\n",
    "elif aux_to_use == aux_data_6:\n",
    "    aux_label = 'BORN_HISP_MEX'\n",
    "elif aux_to_use == aux_data_7:\n",
    "    aux_label = 'BORN_HISP'\n",
    "\n",
    "proj_dir = '/Volumes/Stockage/alex/hrs_1000G/projections'\n",
    "html_dir = '/Volumes/Stockage/alex/hrs_1000G/html'\n",
    "\n",
    "for fname in os.listdir(proj_dir):\n",
    "    if os.path.isdir(os.path.join(proj_dir, fname)) or fname=='merged_1000G_HRS_pca.eigenvec':\n",
    "        continue\n",
    "    else:\n",
    "        temp_proj = np.loadtxt(os.path.join(proj_dir, fname))\n",
    "\n",
    "        num_pcs = fname.split('PC')[1].split('_')[0]\n",
    "        num_nn = fname.split('NN')[1].split('_')[0]\n",
    "        md = fname.split('MD')[1].split('_')[0]\n",
    "\n",
    "        proj_data_hrs = temp_proj[0:(len(aux_data_hrs)-1),:]\n",
    "        proj_data_1000g = temp_proj[len(aux_data_hrs)-1:,:]\n",
    "\n",
    "        p = figure(plot_width=1500, plot_height=1200)\n",
    "        p.title.text = 'PCs: ' + num_pcs + ', NN: ' + num_nn + ', MD: ' + md\n",
    "\n",
    "        # First loop is for HRS data\n",
    "        for pop in sorted(eth_proxy_set):\n",
    "            proj_pop = proj_data_hrs[indices_of_population_members_hrs[pop]]\n",
    "            p.circle(proj_pop[:,0], proj_pop[:,1], legend=pop_dict[pop],\n",
    "                     color = color_dict_hrs[pop])\n",
    "\n",
    "        # Second loop is for 1000G data\n",
    "        for cont in continents: \n",
    "            for pop in pop_by_continent[cont]:\n",
    "                proj_pop = proj_data_1000g[indices_of_population_members[pop]]\n",
    "                p.circle(proj_pop[:,0], proj_pop[:,1], \n",
    "                         legend=name_by_code[pop], color = color_dict[pop])\n",
    "\n",
    "        p.legend.location = \"top_left\"\n",
    "\n",
    "        p.legend.click_policy=\"hide\"\n",
    "\n",
    "        output_file(os.path.join(html_dir, fname + '_' + aux_label + '.html'),title='UMAP_'+fname)\n",
    "\n",
    "        save(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Volumes/Stockage/alex/hrs_1000G/html/HRS_1000G_UMAP_PC10_NC2_NN15_MD0.5_2018627203416_RACE_HISP_MEX.html'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One-off creations\n",
    "# Labels for HRS\n",
    "if aux_to_use == aux_data_1:\n",
    "    aux_label = 'BORN_RACE_HISP_MEX'\n",
    "elif aux_to_use == aux_data_2:\n",
    "    aux_label = 'RACE_HISP_MEX'\n",
    "elif aux_to_use == aux_data_3:\n",
    "    aux_label = 'BORN_RACE'\n",
    "elif aux_to_use == aux_data_4:\n",
    "    aux_label = 'RACE_HISP'\n",
    "elif aux_to_use == aux_data_5:\n",
    "    aux_label = 'BORN'\n",
    "elif aux_to_use == aux_data_6:\n",
    "    aux_label = 'BORN_HISP_MEX'\n",
    "elif aux_to_use == aux_data_7:\n",
    "    aux_label = 'BORN_HISP'\n",
    "\n",
    "proj_dir = '/Volumes/Stockage/alex/hrs_1000G/projections'\n",
    "html_dir = '/Volumes/Stockage/alex/hrs_1000G/html'\n",
    "\n",
    "fname = 'HRS_1000G_UMAP_PC10_NC2_NN15_MD0.5_2018627203416'\n",
    "\n",
    "temp_proj = np.loadtxt(os.path.join(proj_dir, fname))\n",
    "\n",
    "num_pcs = fname.split('PC')[1].split('_')[0]\n",
    "num_nn = fname.split('NN')[1].split('_')[0]\n",
    "md = fname.split('MD')[1].split('_')[0]\n",
    "\n",
    "proj_data_hrs = temp_proj[0:(len(aux_data_hrs)-1),:]\n",
    "proj_data_1000g = temp_proj[len(aux_data_hrs)-1:,:]\n",
    "\n",
    "p = figure(plot_width=1500, plot_height=1200)\n",
    "p.title.text = 'PCs: ' + num_pcs + ', NN: ' + num_nn + ', MD: ' + md\n",
    "\n",
    "# First loop is for HRS data\n",
    "for pop in sorted(eth_proxy_set):\n",
    "    proj_pop = proj_data_hrs[indices_of_population_members_hrs[pop]]\n",
    "    p.circle(proj_pop[:,0], proj_pop[:,1], legend=pop_dict[pop],\n",
    "             color = color_dict_hrs[pop])\n",
    "\n",
    "# Second loop is for 1000G data\n",
    "for cont in continents: \n",
    "    for pop in pop_by_continent[cont]:\n",
    "        proj_pop = proj_data_1000g[indices_of_population_members[pop]]\n",
    "        p.circle(proj_pop[:,0], proj_pop[:,1], \n",
    "                 legend=name_by_code[pop], color = color_dict[pop])\n",
    "\n",
    "p.legend.location = \"top_left\"\n",
    "\n",
    "p.legend.click_policy=\"hide\"\n",
    "\n",
    "output_file(os.path.join(html_dir, fname + '_' + aux_label + '.html'),title='UMAP_'+fname)\n",
    "\n",
    "save(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create non-interactive images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-off creations\n",
    "# Labels for HRS\n",
    "if aux_to_use == aux_data_1:\n",
    "    aux_label = 'BORN_RACE_HISP_MEX'\n",
    "elif aux_to_use == aux_data_2:\n",
    "    aux_label = 'RACE_HISP_MEX'\n",
    "elif aux_to_use == aux_data_3:\n",
    "    aux_label = 'BORN_RACE'\n",
    "elif aux_to_use == aux_data_4:\n",
    "    aux_label = 'RACE_HISP'\n",
    "elif aux_to_use == aux_data_5:\n",
    "    aux_label = 'BORN'\n",
    "elif aux_to_use == aux_data_6:\n",
    "    aux_label = 'BORN_HISP_MEX'\n",
    "elif aux_to_use == aux_data_7:\n",
    "    aux_label = 'BORN_HISP'\n",
    "\n",
    "proj_dir = '/Volumes/Stockage/alex/hrs_1000G/projections'\n",
    "out_dir = '/Volumes/Stockage/alex/hrs_1000G/images'\n",
    "\n",
    "fname = 'HRS_1000G_UMAP_PC10_NC2_NN15_MD0.5_2018627203416'\n",
    "\n",
    "temp_proj = np.loadtxt(os.path.join(proj_dir, fname))\n",
    "\n",
    "num_pcs = fname.split('PC')[1].split('_')[0]\n",
    "num_nn = fname.split('NN')[1].split('_')[0]\n",
    "md = fname.split('MD')[1].split('_')[0]\n",
    "\n",
    "proj_data_hrs = temp_proj[0:(len(aux_data_hrs)-1),:]\n",
    "proj_data_1000g = temp_proj[len(aux_data_hrs)-1:,:]\n",
    "\n",
    "#####\n",
    "\n",
    "fig = plt.figure(figsize=(20,20))\n",
    "ax = fig.add_subplot(111, aspect=1)\n",
    "\n",
    "# First loop is for HRS data\n",
    "for pop in sorted(eth_proxy_set):\n",
    "    temp_proj = proj_data_hrs[indices_of_population_members_hrs[pop]]\n",
    "    ax.scatter(temp_proj[:,0], temp_proj[:,1], label=pop_dict[pop], alpha=0.6, color=color_dict_hrs[pop])\n",
    "\n",
    "# Second loop is for 1000G data\n",
    "for cont in continents: \n",
    "    for pop in pop_by_continent[cont]:\n",
    "        temp_proj = proj_data_1000g[indices_of_population_members[pop]]\n",
    "        ax.scatter(temp_proj[:,0], temp_proj[:,1], label=name_by_code[pop], alpha=0.6, color=color_dict[pop])\n",
    "    \n",
    "ax.legend(ncol=3,loc='lower center', bbox_to_anchor=(0.55,-0.2), fontsize=12,markerscale=3)\n",
    "\n",
    "fig.savefig(os.path.join(out_dir, fname + '.jpeg'),format='jpeg')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sandbox"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the code used to generated PCA projections:\n",
    "\n",
    "#!/bin/bash\n",
    "\n",
    "#PBS -q sw\n",
    "#PBS -l nodes=1:ppn=4\n",
    "\n",
    "#PBS -l walltime=1:00:00\n",
    "#PBS -N common_SNPs_HRS_1000G\n",
    "\n",
    "cd /home/adiazpop/Ethnicity/HRS/data\n",
    "\n",
    "sort merged_1000G_HRS.bim > merged_1000G_HRS_sorted.bim\n",
    "sort new_data_in_paper.bim > new_data_in_paper_sorted.bim\n",
    "\n",
    "comm -12 merged_1000G_HRS_sorted.bim new_data_in_paper_sorted.bim > common_snps_HRS_1000G_Jewish.txt\n",
    "\n",
    "exit\n",
    "\n",
    "#!/bin/bash\n",
    "\n",
    "#PBS -q sw\n",
    "#PBS -l nodes=1:ppn=10\n",
    "\n",
    "#PBS -l walltime=3:00:00\n",
    "#PBS -N merge_HRS_1000G_Jewish\n",
    "\n",
    "cd /home/adiazpop/Ethnicity/HRS/data\n",
    "\n",
    "plink --bfile merged_1000G_HRS --extract common_snps_HRS_1000G_Jewish.txt --make-bed --out merged_1000G_HRS_aj_common\n",
    "plink --bfile new_data_in_paper --extract common_snps_HRS_1000G_Jewish.txt --make-bed --out aj_common\n",
    "plink --bfile merged_1000G_HRS_aj_common --bmerge aj_common --make-bed --out merged_1000G_HRS_Jewish\n",
    "\n",
    "exit\n",
    "\n",
    "#!/bin/bash\n",
    "\n",
    "#PBS -q sw\n",
    "#PBS -l nodes=1:ppn=30\n",
    "\n",
    "#PBS -l walltime=3:00:00\n",
    "#PBS -N HRS_1000G_AJ_PCA\n",
    "\n",
    "cd /home/adiazpop/Ethnicity/HRS/data\n",
    "\n",
    "plink --bfile merged_1000G_HRS_aj_common --pca 50 --out merged_1000G_HRS_aj_pca\n",
    "\n",
    "exit"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
