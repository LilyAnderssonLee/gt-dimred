{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import collections\n",
    "import gzip\n",
    "import numpy as np\n",
    "import os\n",
    "import phate\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import pickle as pk\n",
    "\n",
    "from collections import defaultdict\n",
    "import itertools\n",
    "\n",
    "# Interactive HTML tools\n",
    "from ipywidgets import interact\n",
    "import bokeh\n",
    "import bokeh.io\n",
    "from bokeh.io import push_notebook\n",
    "from bokeh.plotting import figure, show, save, output_notebook, output_file\n",
    "from bokeh.palettes import Category20b\n",
    "from bokeh.palettes import Category20\n",
    "from bokeh.palettes import Category10\n",
    "from bokeh.palettes import PRGn\n",
    "from bokeh.palettes import Set1\n",
    "\n",
    "# Machine-learning and dimensionality reduction tools\n",
    "import sklearn\n",
    "from sklearn import decomposition\n",
    "from sklearn.decomposition import PCA as PCA # We'll use this to check our implementation\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.manifold import MDS\n",
    "\n",
    "import umap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Note: PLINK command used to merged data\n",
    "# sort nhgri_coriell_affy_6_20140825_genotypes.bim > nhgri_coriell_affy_6_20140825_genotypes_sorted.bim\n",
    "# sort HRS_CLEAR.bim > HRS_CLEAR_sorted.bim\n",
    "\n",
    "# comm -12 nhgri_coriell_affy_6_20140825_genotypes_sorted.bim HRS_CLEAR_sorted.bim > common_snps_2.txt\n",
    "# plink --bfile merged_1000G_HRS --pca 200 --out merged_1000G_HRS_pca\n",
    "\n",
    "data_dir = '/Volumes/Stockage/alex/hrs_1000G/projections'\n",
    "\n",
    "# Define the files we'll be using\n",
    "pc_file = 'merged_1000G_HRS_pca.eigenvec'\n",
    "\n",
    "#aux_path = os.path.join(hrs_data_dir, aux_file)\n",
    "pc_path = os.path.join(data_dir, pc_file)\n",
    "\n",
    "# Import auxiliary data. Contains IDs and demographic information.\n",
    "# NOTE: The auxiliary data is sorted in an order different from the PC data.\n",
    "#aux_data = []\n",
    "#with open(aux_path) as input_file:\n",
    "#    for line in input_file:\n",
    "#        aux_data.append(line.strip().split(','))\n",
    "\n",
    "# Import PC data. This data must be converted to an array.\n",
    "with open(pc_path) as pc:\n",
    "    pca_contents = pc.readlines()\n",
    "\n",
    "pca_data = []\n",
    "\n",
    "for pc in pca_contents:\n",
    "    pca_data.append(pc.split()[2:len(pc)])\n",
    "\n",
    "pca_data_array = np.array(pca_data).astype(np.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_data_array.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing auxiliary data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For labelling we need to import auxiliary dataset from the 1000G and HRS datasets. The 1000G dataset is straightforward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import 1000G auxiliary data\n",
    "%store -r continents\n",
    "%store -r pop_by_continent\n",
    "%store -r pop\n",
    "%store -r indices_of_population_members\n",
    "%store -r name_by_code\n",
    "%store -r continent_by_population\n",
    "%store -r individuals\n",
    "%store -r population_by_individual\n",
    "%store -r individuals_by_population\n",
    "%store -r populations\n",
    "%store -r color_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The HRS data needs a bit more work as the variables aren't explicitly defined. We can import the dataset from the HRS notebook and then define values here as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import HRS auxiliary data\n",
    "%store -r aux_data_hrs\n",
    "%store -r hisp_dict\n",
    "%store -r hisp_dict_rev\n",
    "%store -r mex_dict\n",
    "%store -r mex_dict_rev\n",
    "%store -r race_dict\n",
    "%store -r race_dict_rev\n",
    "%store -r brn_dict\n",
    "%store -r brn_dict_rev\n",
    "%store -r racedb_dict\n",
    "%store -r racedb_dict_rev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#aux_data_hrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Labels for the HRS data aren't as clear - import the code to define those as well.\n",
    "# These auxiliary data sets will define how we label the observations\n",
    "\n",
    "# Columns are:\n",
    "# 0 = ID, 1 = Family ID, 2 = Birth Year\n",
    "# 3 = Hispanic, 4 = Detailed Hispanic, 5 = Race, 6 = Birth Region, 7 = Birth region name\n",
    "# 10= dbGaP race (Note: Black != AfrAm and White != Not_AfrAm)\n",
    "\n",
    "# Create multiple types of categorization based on variables to include\n",
    "aux_data_1 = [] # 1 - Birth region, race, Hispanic status, Mexican status\n",
    "aux_data_2 = [] # 2 - Race, Hispanic status, Mexican status\n",
    "aux_data_3 = [] # 3 - Birth region, race\n",
    "aux_data_4 = [] # 4 - Race, Hispanic status\n",
    "aux_data_5 = [] # 5 - Birth region\n",
    "aux_data_6 = [] # 6 - Birth region, Hispanic status, Mexican status\n",
    "aux_data_7 = [] # 7 - Birth region, Hispanic status\n",
    "\n",
    "individuals_hrs = []\n",
    "\n",
    "aux_data_dict_1 = defaultdict(list)\n",
    "aux_data_dict_2 = defaultdict(list)\n",
    "aux_data_dict_3 = defaultdict(list)\n",
    "aux_data_dict_4 = defaultdict(list)\n",
    "aux_data_dict_5 = defaultdict(list)\n",
    "aux_data_dict_6 = defaultdict(list)\n",
    "aux_data_dict_7 = defaultdict(list)\n",
    "\n",
    "# Get the lists (skip the first row as it's a header)\n",
    "for a in aux_data_hrs[1:]:\n",
    "#for a in aux_data_dict[subset][0:]:\n",
    "    individuals_hrs.append(a[0])\n",
    "    \n",
    "    temp_element = [a[0], '_'.join([brn_dict[a[7]], race_dict[a[5]], hisp_dict[a[3]], mex_dict[a[4]]])]    \n",
    "    aux_data_1.append(temp_element)\n",
    "    \n",
    "    temp_element = [a[0], '_'.join([race_dict[a[5]], hisp_dict[a[3]], mex_dict[a[4]]])]\n",
    "    aux_data_2.append(temp_element)\n",
    "    \n",
    "    temp_element = [a[0], '_'.join([brn_dict[a[7]], race_dict[a[5]]])]\n",
    "    aux_data_3.append(temp_element)\n",
    "    \n",
    "    temp_element = [a[0], '_'.join([race_dict[a[5]], hisp_dict[a[3]]])]\n",
    "    aux_data_4.append(temp_element)\n",
    "    \n",
    "    temp_element = [a[0], '_'.join([brn_dict[a[7]]])]\n",
    "    aux_data_5.append(temp_element)\n",
    "    \n",
    "    temp_element = [a[0], '_'.join([brn_dict[a[7]], hisp_dict[a[3]], mex_dict[a[4]]])]\n",
    "    aux_data_6.append(temp_element)\n",
    "    \n",
    "    temp_element = [a[0], '_'.join([brn_dict[a[7]], hisp_dict[a[3]]])]\n",
    "    aux_data_7.append(temp_element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "aux_to_use = aux_data_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We must define the population dictionary we wish to use\n",
    "# The following gives us a collection of all categories of some population and/or proxy for ethnicity:\n",
    "eth_proxy_set = set([a[1] for a in aux_to_use])\n",
    "pop_dict = dict()\n",
    "\n",
    "for e in eth_proxy_set:\n",
    "    el = e.split('_')\n",
    "    \n",
    "    if aux_to_use == aux_data_1:    \n",
    "        temp_brn = brn_dict_rev[el[0]]\n",
    "        temp_race = race_dict_rev[el[1]]\n",
    "        temp_hisp = hisp_dict_rev[el[2]]\n",
    "        temp_mex = mex_dict_rev[el[3]]\n",
    "        \n",
    "        pop_dict.update({e:temp_brn + ' ' + temp_race + ' ' + temp_hisp + ' ' + temp_mex})\n",
    "    elif aux_to_use == aux_data_2:\n",
    "        temp_race = race_dict_rev[el[0]]\n",
    "        temp_hisp = hisp_dict_rev[el[1]]\n",
    "        temp_mex = mex_dict_rev[el[2]]\n",
    "        temp_list = [temp_race, temp_hisp, temp_mex]\n",
    "        \n",
    "        pop_dict.update({e:temp_race + ' ' + temp_hisp + ' ' + temp_mex})\n",
    "    elif aux_to_use == aux_data_3:\n",
    "        temp_brn = brn_dict_rev[el[0]]\n",
    "        temp_race = race_dict_rev[el[1]]\n",
    "        \n",
    "        pop_dict.update({e: temp_brn + ' ' + temp_race})\n",
    "    elif aux_to_use == aux_data_4:\n",
    "        temp_race = race_dict_rev[el[0]]\n",
    "        temp_hisp = hisp_dict_rev[el[1]]\n",
    "        \n",
    "        pop_dict.update({e: temp_race + ' ' + temp_hisp})\n",
    "    elif aux_to_use == aux_data_5:\n",
    "        temp_brn = brn_dict_rev[el[0]]\n",
    "        \n",
    "        pop_dict.update({e: temp_brn})\n",
    "    elif aux_to_use == aux_data_6:\n",
    "        temp_brn = brn_dict_rev[el[0]]\n",
    "        temp_hisp = hisp_dict_rev[el[1]]\n",
    "        temp_mex = mex_dict_rev[el[2]]\n",
    "        \n",
    "        pop_dict.update({e: temp_brn + ' ' + temp_hisp + ' ' + temp_mex})\n",
    "    elif aux_to_use == aux_data_7:\n",
    "        temp_brn = brn_dict_rev[el[0]]\n",
    "        temp_hisp = hisp_dict_rev[el[1]]\n",
    "        \n",
    "        pop_dict.update({e: temp_brn + ' ' + temp_hisp})\n",
    "    \n",
    "    #print(el, temp_list)\n",
    "    #print(e.split('_'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set up an index of each population member and vice versa\n",
    "# We want to quickly access a given individual's population and a given population's individuals\n",
    "population_by_individual_hrs = defaultdict(int)\n",
    "individuals_by_population_hrs = defaultdict(list)\n",
    "\n",
    "for a in aux_to_use:\n",
    "    population_by_individual_hrs[a[0]] = a[1]\n",
    "    individuals_by_population_hrs[a[1]].append(a[0])\n",
    "    \n",
    "indices_of_population_members_hrs = defaultdict(list)\n",
    "\n",
    "for index, indiv in enumerate(individuals_hrs):\n",
    "    try:\n",
    "        indices_of_population_members_hrs[population_by_individual_hrs[indiv]].append(index)\n",
    "    except KeyError:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting projections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Need to define colour dictionaries - just go with what was previously set up\n",
    "color_dict_1000g = {}\n",
    "for i, cont in enumerate(continents): \n",
    "    for j, pop in enumerate(pop_by_continent[cont]):\n",
    "        color_dict_1000g[pop] = Category20b[20][4*i+j%4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eth_proxy_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%store -r color_dict_born\n",
    "%store -r color_dict_race_hisp\n",
    "%store -r color_dict_race_hisp_mex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "color_dict_hrs = color_dict_race_hisp_mex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Directory where projection coordinates are stored\n",
    "proj_dir = '/Volumes/Stockage/alex/hrs_1000G/projections'\n",
    "\n",
    "# Labels for HRS\n",
    "if aux_to_use == aux_data_1:\n",
    "    aux_label = 'BORN_RACE_HISP_MEX'\n",
    "elif aux_to_use == aux_data_2:\n",
    "    aux_label = 'RACE_HISP_MEX'\n",
    "elif aux_to_use == aux_data_3:\n",
    "    aux_label = 'BORN_RACE'\n",
    "elif aux_to_use == aux_data_4:\n",
    "    aux_label = 'RACE_HISP'\n",
    "elif aux_to_use == aux_data_5:\n",
    "    aux_label = 'BORN'\n",
    "elif aux_to_use == aux_data_6:\n",
    "    aux_label = 'BORN_HISP_MEX'\n",
    "elif aux_to_use == aux_data_7:\n",
    "    aux_label = 'BORN_HISP'\n",
    "\n",
    "# Loop though each tSNE projection\n",
    "for file in os.listdir(proj_dir):\n",
    "    num_pcs = file.split('PC_')[1].split('_')[0]\n",
    "    plex = file.split('PLEX_')[1]\n",
    "    \n",
    "    ptitle_str = 'HRS+1000G tSNE on ' + str(num_pcs) + ' PCs, perplexity ' + str(plex)\n",
    "    fname_str = 'TSNE_HRS_1000G_' + aux_label + '_PC_' + str(num_pcs) + '_PLEX_' + str(plex)\n",
    "    ftitle_str = 'tSNE on HRS+1000G data, ' + str(num_pcs) + ' PCs, perplexity ' + str(plex)\n",
    "    arr_str = os.path.join(proj_dir,'HRS_1000G_TSNE_PC_' + str(num_pcs) + '_PLEX_' + str(plex))\n",
    "    \n",
    "    int_html_hrs_1000g(ptitle_str, fname_str, ftitle_str, arr_str)\n",
    "    print('Finished created interactive HTML file on ' + str(num_pcs) + ' PCs with perplexity ' + str(plex))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a custom function that creates interactive HTML based on file input\n",
    "# Inputs: Plot title, file name, file title, num_pcs, perplexity, array file location\n",
    "def int_html_hrs_1000g(ptitle, fname, ftitle, arr):\n",
    "    component_1_id = 0\n",
    "    component_2_id = 1\n",
    "\n",
    "    proj_data = np.loadtxt(arr)\n",
    "\n",
    "    proj_data_hrs = proj_data[0:(len(aux_data_hrs)-1),:]\n",
    "    proj_data_1000g = proj_data[len(aux_data_hrs)-1:,:]\n",
    "    \n",
    "    p = figure(plot_width=1500, plot_height=1200)\n",
    "    p.title.text = ptitle\n",
    "\n",
    "    # First loop is for HRS data\n",
    "    for pop in sorted(eth_proxy_set):\n",
    "        proj_pop = proj_data_hrs[indices_of_population_members_hrs[pop]]\n",
    "        p.circle(proj_pop[:,component_1_id], proj_pop[:,component_2_id], legend=pop_dict[pop],\n",
    "                 color = color_dict_hrs[pop])\n",
    "\n",
    "    # Second loop is for 1000G data\n",
    "    for cont in continents: \n",
    "        for pop in pop_by_continent[cont]:\n",
    "            proj_pop = proj_data_1000g[indices_of_population_members[pop]]\n",
    "            p.circle(proj_pop[:,component_1_id], proj_pop[:,component_2_id], \n",
    "                     legend=name_by_code[pop], color = color_dict[pop])\n",
    "\n",
    "    p.legend.location = \"top_left\"\n",
    "\n",
    "    p.legend.click_policy=\"hide\"\n",
    "\n",
    "    output_file(fname + '.html',title=ftitle)\n",
    "\n",
    "    save(p)\n",
    "\n",
    "def int_html_hrs_1000g_local(ptitle, fname, ftitle, arr):\n",
    "    component_1_id = 0\n",
    "    component_2_id = 1\n",
    "\n",
    "    proj_data = arr\n",
    "\n",
    "    proj_data_hrs = proj_data[0:(len(aux_data_hrs)-1),:]\n",
    "    proj_data_1000g = proj_data[len(aux_data_hrs)-1:,:]\n",
    "    \n",
    "    p = figure(plot_width=1500, plot_height=1200)\n",
    "    p.title.text = ptitle\n",
    "\n",
    "    # First loop is for HRS data\n",
    "    for pop in sorted(eth_proxy_set):\n",
    "        proj_pop = proj_data_hrs[indices_of_population_members_hrs[pop]]\n",
    "        p.circle(proj_pop[:,component_1_id], proj_pop[:,component_2_id], legend=pop_dict[pop],\n",
    "                 color = color_dict_hrs[pop])\n",
    "\n",
    "    # Second loop is for 1000G data\n",
    "    for cont in continents: \n",
    "        for pop in pop_by_continent[cont]:\n",
    "            proj_pop = proj_data_1000g[indices_of_population_members[pop]]\n",
    "            p.circle(proj_pop[:,component_1_id], proj_pop[:,component_2_id], \n",
    "                     legend=name_by_code[pop], color = color_dict[pop])\n",
    "\n",
    "    p.legend.location = \"top_left\"\n",
    "\n",
    "    p.legend.click_policy=\"hide\"\n",
    "\n",
    "    output_file(fname + '.html',title=ftitle)\n",
    "\n",
    "    save(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "int_html_hrs_1000g_local('test','test','test',mds_proj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create HTML of UMAP projections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Labels for HRS\n",
    "if aux_to_use == aux_data_1:\n",
    "    aux_label = 'BORN_RACE_HISP_MEX'\n",
    "elif aux_to_use == aux_data_2:\n",
    "    aux_label = 'RACE_HISP_MEX'\n",
    "elif aux_to_use == aux_data_3:\n",
    "    aux_label = 'BORN_RACE'\n",
    "elif aux_to_use == aux_data_4:\n",
    "    aux_label = 'RACE_HISP'\n",
    "elif aux_to_use == aux_data_5:\n",
    "    aux_label = 'BORN'\n",
    "elif aux_to_use == aux_data_6:\n",
    "    aux_label = 'BORN_HISP_MEX'\n",
    "elif aux_to_use == aux_data_7:\n",
    "    aux_label = 'BORN_HISP'\n",
    "\n",
    "proj_dir = '/Volumes/Stockage/alex/hrs_1000G/projections'\n",
    "html_dir = '/Volumes/Stockage/alex/hrs_1000G/html'\n",
    "\n",
    "for fname in os.listdir(proj_dir):\n",
    "    if os.path.isdir(os.path.join(proj_dir, fname)) or fname=='merged_1000G_HRS_pca.eigenvec':\n",
    "        continue\n",
    "    else:\n",
    "        temp_proj = np.loadtxt(os.path.join(proj_dir, fname))\n",
    "\n",
    "        num_pcs = fname.split('PC')[1].split('_')[0]\n",
    "        num_nn = fname.split('NN')[1].split('_')[0]\n",
    "        md = fname.split('MD')[1].split('_')[0]\n",
    "\n",
    "        proj_data_hrs = temp_proj[0:(len(aux_data_hrs)-1),:]\n",
    "        proj_data_1000g = temp_proj[len(aux_data_hrs)-1:,:]\n",
    "\n",
    "        p = figure(plot_width=1500, plot_height=1200)\n",
    "        p.title.text = 'PCs: ' + num_pcs + ', NN: ' + num_nn + ', MD: ' + md\n",
    "\n",
    "        # First loop is for HRS data\n",
    "        for pop in sorted(eth_proxy_set):\n",
    "            proj_pop = proj_data_hrs[indices_of_population_members_hrs[pop]]\n",
    "            p.circle(proj_pop[:,0], proj_pop[:,1], legend=pop_dict[pop],\n",
    "                     color = color_dict_hrs[pop])\n",
    "\n",
    "        # Second loop is for 1000G data\n",
    "        for cont in continents: \n",
    "            for pop in pop_by_continent[cont]:\n",
    "                proj_pop = proj_data_1000g[indices_of_population_members[pop]]\n",
    "                p.circle(proj_pop[:,0], proj_pop[:,1], \n",
    "                         legend=name_by_code[pop], color = color_dict[pop])\n",
    "\n",
    "        p.legend.location = \"top_left\"\n",
    "\n",
    "        p.legend.click_policy=\"hide\"\n",
    "\n",
    "        output_file(os.path.join(html_dir, fname + '_' + aux_label + '.html'),title='UMAP_'+fname)\n",
    "\n",
    "        save(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-off creations\n",
    "# Labels for HRS\n",
    "if aux_to_use == aux_data_1:\n",
    "    aux_label = 'BORN_RACE_HISP_MEX'\n",
    "elif aux_to_use == aux_data_2:\n",
    "    aux_label = 'RACE_HISP_MEX'\n",
    "elif aux_to_use == aux_data_3:\n",
    "    aux_label = 'BORN_RACE'\n",
    "elif aux_to_use == aux_data_4:\n",
    "    aux_label = 'RACE_HISP'\n",
    "elif aux_to_use == aux_data_5:\n",
    "    aux_label = 'BORN'\n",
    "elif aux_to_use == aux_data_6:\n",
    "    aux_label = 'BORN_HISP_MEX'\n",
    "elif aux_to_use == aux_data_7:\n",
    "    aux_label = 'BORN_HISP'\n",
    "\n",
    "proj_dir = '/Volumes/Stockage/alex/hrs_1000G/projections'\n",
    "html_dir = '/Volumes/Stockage/alex/hrs_1000G/html'\n",
    "\n",
    "fname = 'HRS_1000G_UMAP_PC10_NC2_NN15_MD0.5_2018627203416'\n",
    "\n",
    "temp_proj = np.loadtxt(os.path.join(proj_dir, fname))\n",
    "\n",
    "num_pcs = fname.split('PC')[1].split('_')[0]\n",
    "num_nn = fname.split('NN')[1].split('_')[0]\n",
    "md = fname.split('MD')[1].split('_')[0]\n",
    "\n",
    "proj_data_hrs = temp_proj[0:(len(aux_data_hrs)-1),:]\n",
    "proj_data_1000g = temp_proj[len(aux_data_hrs)-1:,:]\n",
    "\n",
    "p = figure(plot_width=1500, plot_height=1200)\n",
    "p.title.text = 'PCs: ' + num_pcs + ', NN: ' + num_nn + ', MD: ' + md\n",
    "\n",
    "# First loop is for HRS data\n",
    "for pop in sorted(eth_proxy_set):\n",
    "    proj_pop = proj_data_hrs[indices_of_population_members_hrs[pop]]\n",
    "    p.circle(proj_pop[:,0], proj_pop[:,1], legend=pop_dict[pop],\n",
    "             color = color_dict_hrs[pop])\n",
    "\n",
    "# Second loop is for 1000G data\n",
    "for cont in continents: \n",
    "    for pop in pop_by_continent[cont]:\n",
    "        proj_pop = proj_data_1000g[indices_of_population_members[pop]]\n",
    "        p.circle(proj_pop[:,0], proj_pop[:,1], \n",
    "                 legend=name_by_code[pop], color = color_dict[pop])\n",
    "\n",
    "p.legend.location = \"top_left\"\n",
    "\n",
    "p.legend.click_policy=\"hide\"\n",
    "\n",
    "output_file(os.path.join(html_dir, fname + '_' + aux_label + '.html'),title='UMAP_'+fname)\n",
    "\n",
    "save(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create non-interactive images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# One-off creations\n",
    "# Labels for HRS\n",
    "if aux_to_use == aux_data_1:\n",
    "    aux_label = 'BORN_RACE_HISP_MEX'\n",
    "elif aux_to_use == aux_data_2:\n",
    "    aux_label = 'RACE_HISP_MEX'\n",
    "elif aux_to_use == aux_data_3:\n",
    "    aux_label = 'BORN_RACE'\n",
    "elif aux_to_use == aux_data_4:\n",
    "    aux_label = 'RACE_HISP'\n",
    "elif aux_to_use == aux_data_5:\n",
    "    aux_label = 'BORN'\n",
    "elif aux_to_use == aux_data_6:\n",
    "    aux_label = 'BORN_HISP_MEX'\n",
    "elif aux_to_use == aux_data_7:\n",
    "    aux_label = 'BORN_HISP'\n",
    "\n",
    "proj_dir = '/Volumes/Stockage/alex/hrs_1000G/projections'\n",
    "out_dir = '/Volumes/Stockage/alex/hrs_1000G/images'\n",
    "\n",
    "fname = 'HRS_1000G_UMAP_PC10_NC2_NN15_MD0.5_2018627203416'\n",
    "\n",
    "temp_proj = np.loadtxt(os.path.join(proj_dir, fname))\n",
    "\n",
    "num_pcs = fname.split('PC')[1].split('_')[0]\n",
    "num_nn = fname.split('NN')[1].split('_')[0]\n",
    "md = fname.split('MD')[1].split('_')[0]\n",
    "\n",
    "proj_data_hrs = temp_proj[0:(len(aux_data_hrs)-1),:]\n",
    "proj_data_1000g = temp_proj[len(aux_data_hrs)-1:,:]\n",
    "\n",
    "#####\n",
    "\n",
    "fig = plt.figure(figsize=(30,30))\n",
    "ax = fig.add_subplot(111, aspect=1)\n",
    "\n",
    "# First loop is for HRS data\n",
    "for pop in sorted(eth_proxy_set):\n",
    "    temp_proj = proj_data_hrs[indices_of_population_members_hrs[pop]]\n",
    "    ax.scatter(temp_proj[:,0], temp_proj[:,1], label=pop_dict[pop], alpha=0.6, color=color_dict_hrs[pop])\n",
    "\n",
    "# Second loop is for 1000G data\n",
    "for cont in continents: \n",
    "    for pop in pop_by_continent[cont]:\n",
    "        temp_proj = proj_data_1000g[indices_of_population_members[pop]]\n",
    "        ax.scatter(temp_proj[:,0], temp_proj[:,1], label=name_by_code[pop], alpha=0.6, color=color_dict[pop])\n",
    "    \n",
    "ax.legend(ncol=3,loc='lower center', bbox_to_anchor=(0.55,-0.2), fontsize=12,markerscale=3)\n",
    "\n",
    "fig.savefig(os.path.join(out_dir, fname + '.jpeg'),format='jpeg')\n",
    "plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
