{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "This notebook assumes that you have already dimensionally reduced your data and is intended to create visualizations or subset data in interesting ways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import collections\n",
    "import gzip\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from collections import defaultdict\n",
    "import itertools\n",
    "\n",
    "# Interactive HTML tools\n",
    "from ipywidgets import interact\n",
    "import bokeh\n",
    "import bokeh.io\n",
    "from bokeh.io import push_notebook\n",
    "from bokeh.plotting import figure, show, save, output_notebook, output_file\n",
    "from bokeh.palettes import Category20b\n",
    "from bokeh.palettes import Category10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes\n",
    "\n",
    "* QC reports on genotype data available here: https://hrs.isr.umich.edu/data-products/genetic-data/products#gdv1\n",
    "* PCs were generated in PLINK\n",
    "\n",
    "PLINK command used to create cleaned HRS data:\n",
    "\n",
    "```\n",
    "plink --bfile HRS --maf 0.05 --mind 0.1 --geno 0.1 --hwe 1e-6 --make-bed --out HRS_CLEAR\n",
    "```\n",
    "\n",
    "Relevant parameters: Minor Allele Freq, Missing genotype rates (per-sample) Missing genotype rates (per-variant), \n",
    "Hardy-Weinberg equilibrium\n",
    "\n",
    "PLINK command used to create PCs:\n",
    "```\n",
    "plink --bfile HRS_CLEAR --pca [n]\n",
    "```\n",
    "\n",
    "PLINK command used to create PCs for white-identified individuals in HRS:\n",
    "```\n",
    "plink --bfile HRS_CLEAR --pca [n] --keep hrs_subset_white.txt\n",
    "```\n",
    "\n",
    "This code holds for the hispanic- and black-identified populations as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directories\n",
    "hrs_data_dir = '/Volumes/Stockage/alex/hrs/projections'\n",
    "hrs_aux_dir = '/Volumes/Stockage/alex/hrs/aux'\n",
    "\n",
    "# Auxiliary data\n",
    "aux_file = 'allIndivs_filtered.txt'\n",
    "# Specify the principal component files\n",
    "pc_files = ['plink.eigenvec_200','HRS_PCA_black.eigenvec','HRS_PCA_hispanic.eigenvec','HRS_PCA_white.eigenvec']\n",
    "\n",
    "# Specify which one we want to use\n",
    "pc_file = pc_files[0]\n",
    "\n",
    "aux_path = os.path.join(hrs_aux_dir, aux_file)\n",
    "pc_path = os.path.join(hrs_data_dir, pc_file)\n",
    "\n",
    "# Import auxiliary data. Contains IDs and demographic information.\n",
    "# NOTE: The auxiliary data is sorted in an order different from the PC data.\n",
    "aux_data = []\n",
    "with open(aux_path) as input_file:\n",
    "    for line in input_file:\n",
    "        aux_data.append(line.strip().split(','))\n",
    "\n",
    "# Import PC data. This data must be converted to an array.\n",
    "with open(pc_path) as h:\n",
    "    hrs_contents = h.readlines()\n",
    "\n",
    "hrs_data = []\n",
    "\n",
    "for h in hrs_contents:\n",
    "    hrs_data.append(h.split()[2:len(h)])\n",
    "\n",
    "hrs_data_array = np.array(hrs_data).astype(np.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a dict containing subsets and their related files.\n",
    "# This dict contains an array for each of the population's PC data\n",
    "hrs_subsets_dict = {'All':'plink.eigenvec_200',\n",
    "                    'Black':'HRS_PCA_black.eigenvec',\n",
    "                   'Hispanic':'HRS_PCA_hispanic.eigenvec',\n",
    "                    'White':'HRS_PCA_white.eigenvec'}\n",
    "\n",
    "aux_path = os.path.join(hrs_aux_dir, aux_file)\n",
    "\n",
    "aux_data = []\n",
    "with open(aux_path) as input_file:\n",
    "    for line in input_file:\n",
    "        aux_data.append(line.strip().split(','))\n",
    "\n",
    "hrs_data_dict = defaultdict(np.array)\n",
    "\n",
    "for p in hrs_subsets_dict:\n",
    "    pc_path = os.path.join(hrs_data_dir, hrs_subsets_dict[p])\n",
    "    \n",
    "    # Import PC data. This data must be converted to an array.\n",
    "    with open(pc_path) as h:\n",
    "        hrs_contents = h.readlines()\n",
    "\n",
    "    hrs_data = []\n",
    "        \n",
    "    for h in hrs_contents:\n",
    "        hrs_data.append(h.split()[2:len(h)])\n",
    "\n",
    "    hrs_data_dict[p] = np.array(hrs_data).astype(np.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import admixture proportion estimates\n",
    "admix_dir = hrs_aux_dir\n",
    "\n",
    "# Separated into two datasets (roughly speaking, African Americans and European Americans)\n",
    "admix_data = []\n",
    "\n",
    "# Import data from first file\n",
    "with open(os.path.join(admix_dir,'HRS.txt')) as input_file:\n",
    "    for line in input_file:\n",
    "        admix_data.append(line.strip().split())\n",
    "\n",
    "# Import data from second file\n",
    "with open(os.path.join(admix_dir,'HRS_EurAm.txt')) as input_file:\n",
    "    for line in input_file:\n",
    "        admix_data.append(line.strip().split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Transform this into a Pandas data frame and convert the numeric values to... numeric\n",
    "# ID is stored as numeric as well. This is to match with HRS data, which is sorted numerically by IDs\n",
    "admix_df = pd.DataFrame.from_records(admix_data, columns=['ID','ADMIX1','ADMIX2','ADMIX3'])\n",
    "admix_df[['ADMIX1','ADMIX2','ADMIX3']] = admix_df[['ADMIX1','ADMIX2','ADMIX3']].apply(pd.to_numeric)\n",
    "admix_df['ID'] = admix_df.ID.astype(np.int64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation\n",
    "Data from the HRS is not as neatly organized as in the UKBB or 1KGP. There is no variable for ethnicity, though we can create a proxy by combining variables for self-identified race, Hispanic status, and Mexican-American status. We need to create a variety of dictionaries to deal with these combinations as well. Since we're working with subsets a bit we need to prepare for that too.\n",
    "\n",
    "There are multiple variables that need to be selected:\n",
    "1. Which population to use\n",
    "2. How we want to colour the points (e.g. by birth region, by ethnicity proxy, etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import the data and convert it to a pandas dataset for easier multivariable sorting.\n",
    "# Rename columns (to deal with special characters)\n",
    "hrs_labels = ['IndID','FamID','BirthYear','HispanicStatus','DetailedHispanicStatus','Race_HRS','BirthRegionNum',\n",
    "             'BirthRegionName','AgeRange','Gender','Race_dbGaP']\n",
    "hrs_df = pd.DataFrame.from_records(aux_data[1:],columns = hrs_labels)\n",
    "hrs_df[['FamID','IndID']] = hrs_df[['FamID','IndID']].apply(pd.to_numeric)\n",
    "\n",
    "# Create a new dataset and reset the index\n",
    "hrs_df_sorted = hrs_df.sort_values(by=['FamID','IndID'])\n",
    "hrs_df_sorted = hrs_df_sorted.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Join the admixture data - this gets used at the end for different colourings.\n",
    "hrs_joined = hrs_df_sorted.merge(admix_df, left_on='IndID', right_on='ID', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an auxiliary dataset for each subset\n",
    "# This is necessary so the indices match up when we want to attach a category/label to our points\n",
    "aux_data_dict = defaultdict(list)\n",
    "\n",
    "# Create a subset of our population and add it to the dict\n",
    "for hrs_pop in hrs_subsets_dict:\n",
    "    if hrs_pop in ['White','Black']:\n",
    "        hrs_df_subset = hrs_df_sorted.loc[(hrs_df_sorted['Race_HRS'] == hrs_pop)]\n",
    "    elif hrs_pop in ['Hispanic']:\n",
    "        hrs_df_subset = hrs_df_sorted.loc[(hrs_df_sorted['HispanicStatus'] == 'Hispanic')]\n",
    "    elif hrs_pop == 'All':\n",
    "        hrs_df_subset = hrs_df_sorted\n",
    "    \n",
    "    hrs_df_subset_list = hrs_df_subset.values.tolist()\n",
    "    aux_data_dict[hrs_pop] = hrs_df_subset_list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define our dictionaries.\n",
    "# Create dicts for these values.\n",
    "hisp_dict = {'Hispanic':'H', 'Not_Hispanic':'N'}\n",
    "mex_dict = {'Mexican-American':'M', 'N/A':'N', 'Other':'O', 'Type_Unknown':'U'}\n",
    "race_dict = {'Black':'B', 'Other':'O', 'White':'W'}\n",
    "brn_dict = {'East_North_Central':'ENC', 'East_South_Central':'ESC', 'Middle_Atlantic':'MAT',\n",
    "           'Mountain':'MNT', 'New_England':'ENG', 'Not_In_Contiguous_US':'NIC','Pacific':'PAC',\n",
    "            'South_Atlantic':'SAT', 'West_North_Central':'WNC', 'West_South_Central':'WSC'}\n",
    "racedb_dict = {'AfrAm':'AA', 'Not_AfrAm':'NAA'}\n",
    "\n",
    "# Create reverse lookups. We don't have to use defaultdicts as this is 1-1\n",
    "hisp_dict_rev = dict()\n",
    "mex_dict_rev = dict()\n",
    "race_dict_rev = dict()\n",
    "brn_dict_rev = dict()\n",
    "racedb_dict_rev = dict()\n",
    "\n",
    "for key, value in hisp_dict.items():\n",
    "    hisp_dict_rev.update({value: key})\n",
    "    \n",
    "for key, value in mex_dict.items():\n",
    "    mex_dict_rev.update({value: key})\n",
    "\n",
    "for key, value in race_dict.items():\n",
    "    race_dict_rev.update({value: key})\n",
    "\n",
    "for key, value in brn_dict.items():\n",
    "    brn_dict_rev.update({value: key})\n",
    "    \n",
    "for key, value in racedb_dict.items():\n",
    "    racedb_dict_rev.update({value: key})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select a set of data here:\n",
    "* All\n",
    "* Black\n",
    "* Hispanic\n",
    "* White"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "subset = 'All'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using all entries - no subsetting carried out.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    subset\n",
    "except NameError:\n",
    "    print('Subset not defined. Proceeding with full set of observations.')\n",
    "else:\n",
    "    if subset == 'White':\n",
    "        hrs_subset_indices = hrs_df_sorted.loc[(hrs_df_sorted['Race_HRS'] == subset)].index\n",
    "        hrs_df_sorted = hrs_df_sorted.loc[(hrs_df_sorted['Race_HRS'] == 'White')]\n",
    "        print('Subsetting HRS data: White')\n",
    "    elif subset == 'Black':\n",
    "        hrs_subset_indices = hrs_df_sorted.loc[(hrs_df_sorted['Race_HRS'] == subset)].index\n",
    "        hrs_df_sorted = hrs_df_sorted.loc[(hrs_df_sorted['Race_HRS'] == 'Black')]\n",
    "        print('Subsetting HRS data: Black')\n",
    "    elif subset == 'Hispanic':\n",
    "        hrs_subset_indices = hrs_df_sorted.loc[(hrs_df_sorted['HispanicStatus'] == subset)].index\n",
    "        hrs_df_sorted = hrs_df_sorted.loc[(hrs_df_sorted['HispanicStatus'] == 'Hispanic')]\n",
    "        print('Subsetting HRS data: Hispanic')\n",
    "    elif subset == 'All':\n",
    "        hrs_subset_indices = hrs_df_sorted.index\n",
    "        print('Using all entries - no subsetting carried out.')\n",
    "    else:\n",
    "        print('Subset \"' + str(subset) + '\" not recognized. Using full HRS dataset.')\n",
    "    \n",
    "# Use the sorted dataframe for auxiliary data\n",
    "aux_data_sorted = hrs_df_sorted.values.tolist()\n",
    "aux_data_sorted.insert(0,aux_data[0])\n",
    "\n",
    "aux_data = aux_data_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# These auxiliary data sets will define how we label the observations\n",
    "\n",
    "# Columns are:\n",
    "# 0 = ID, 1 = Family ID, 2 = Birth Year\n",
    "# 3 = Hispanic, 4 = Detailed Hispanic, 5 = Race, 6 = Birth Region, 7 = Birth region name\n",
    "# 10= dbGaP race (Note: Black != AfrAm and White != Not_AfrAm)\n",
    "\n",
    "# Create multiple types of categorization based on variables to include\n",
    "aux_data_1 = [] # 1 - Birth region, race, Hispanic status, Mexican status\n",
    "aux_data_2 = [] # 2 - Race, Hispanic status, Mexican status\n",
    "aux_data_3 = [] # 3 - Birth region, race\n",
    "aux_data_4 = [] # 4 - Race, Hispanic status\n",
    "aux_data_5 = [] # 5 - Birth region\n",
    "aux_data_6 = [] # 6 - Birth region, Hispanic status, Mexican status\n",
    "\n",
    "individuals_hrs = []\n",
    "\n",
    "# Get the lists (skip the first row as it's a header)\n",
    "#for a in aux_data[1:]:\n",
    "for a in aux_data_dict[subset][0:]:\n",
    "    individuals_hrs.append(a[0])\n",
    "    \n",
    "    temp_element = [a[0], '_'.join([brn_dict[a[7]], race_dict[a[5]], hisp_dict[a[3]], mex_dict[a[4]]])]    \n",
    "    aux_data_1.append(temp_element)\n",
    "    \n",
    "    temp_element = [a[0], '_'.join([race_dict[a[5]], hisp_dict[a[3]], mex_dict[a[4]]])]\n",
    "    aux_data_2.append(temp_element)\n",
    "    \n",
    "    temp_element = [a[0], '_'.join([brn_dict[a[7]], race_dict[a[5]]])]\n",
    "    aux_data_3.append(temp_element)\n",
    "    \n",
    "    temp_element = [a[0], '_'.join([race_dict[a[5]], hisp_dict[a[3]]])]\n",
    "    aux_data_4.append(temp_element)\n",
    "    \n",
    "    temp_element = [a[0], '_'.join([brn_dict[a[7]]])]\n",
    "    aux_data_5.append(temp_element)\n",
    "    \n",
    "    temp_element = [a[0], '_'.join([brn_dict[a[7]], hisp_dict[a[3]], mex_dict[a[4]]])]\n",
    "    aux_data_6.append(temp_element)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select which of the auxiliary datasets we will use to label observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "aux_to_use = aux_data_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We must define the population dictionary we wish to use\n",
    "# The following gives us a collection of all categories of some population and/or proxy for ethnicity:\n",
    "eth_proxy_set = set([a[1] for a in aux_to_use])\n",
    "pop_dict = dict()\n",
    "\n",
    "for e in eth_proxy_set:\n",
    "    el = e.split('_')\n",
    "    \n",
    "    if aux_to_use == aux_data_1:    \n",
    "        temp_brn = brn_dict_rev[el[0]]\n",
    "        temp_race = race_dict_rev[el[1]]\n",
    "        temp_hisp = hisp_dict_rev[el[2]]\n",
    "        temp_mex = mex_dict_rev[el[3]]\n",
    "        \n",
    "        pop_dict.update({e:temp_brn + ' ' + temp_race + ' ' + temp_hisp + ' ' + temp_mex})\n",
    "    elif aux_to_use == aux_data_2:\n",
    "        temp_race = race_dict_rev[el[0]]\n",
    "        temp_hisp = hisp_dict_rev[el[1]]\n",
    "        temp_mex = mex_dict_rev[el[2]]\n",
    "        temp_list = [temp_race, temp_hisp, temp_mex]\n",
    "        \n",
    "        pop_dict.update({e:temp_race + ' ' + temp_hisp + ' ' + temp_mex})\n",
    "    elif aux_to_use == aux_data_3:\n",
    "        temp_brn = brn_dict_rev[el[0]]\n",
    "        temp_race = race_dict_rev[el[1]]\n",
    "        \n",
    "        pop_dict.update({e: temp_brn + ' ' + temp_race})\n",
    "    elif aux_to_use == aux_data_4:\n",
    "        temp_race = race_dict_rev[el[0]]\n",
    "        temp_hisp = hisp_dict_rev[el[1]]\n",
    "        \n",
    "        pop_dict.update({e: temp_race + ' ' + temp_hisp})\n",
    "    elif aux_to_use == aux_data_5:\n",
    "        temp_brn = brn_dict_rev[el[0]]\n",
    "        \n",
    "        pop_dict.update({e: temp_brn})\n",
    "    elif aux_to_use == aux_data_6:\n",
    "        temp_brn = brn_dict_rev[el[0]]\n",
    "        temp_hisp = hisp_dict_rev[el[1]]\n",
    "        temp_mex = mex_dict_rev[el[2]]\n",
    "        \n",
    "        pop_dict.update({e: temp_brn + ' ' + temp_hisp + ' ' + temp_mex})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a colour set - this supports up to 30 colours\n",
    "from bokeh.palettes import Category20, Category20b, Category20c, PRGn, Set1\n",
    "\n",
    "color_dict_hrs = {}\n",
    "\n",
    "for j, pop in enumerate(eth_proxy_set):\n",
    "    if j < 20:\n",
    "        color_dict_hrs[pop] = Category20[20][j]\n",
    "    elif j < 30:\n",
    "        color_dict_hrs[pop] = PRGn[10][j%20]\n",
    "    else:\n",
    "        color_dict_hrs[pop] = Set1[9][j%30]\n",
    "\n",
    "# The code from above works okay but sometimes generates different colours for same groups (i.e. inconsistent colours\n",
    "# between our plots)\n",
    "# Define some more specific colour dictionaries for the following three categories:\n",
    "# 1 - Birth region (10)\n",
    "# 2 - Race + Hispanic status (6)\n",
    "# 3 - Race + Hispanic status + Mexican status (8)\n",
    "\n",
    "# Birth regions are US census regions. Five divisions:\n",
    "# WEST: Pacfic (PAC), Mountain (MNT)\n",
    "# MIDWEST: West North Central (WNC), East North Central (ENC)\n",
    "# SOUTH: West South Central (WSC), East South Central (ESC), South Atlantic (SAT)\n",
    "# NORTHEAST: Middle Atlantic (MAT), New England (ENG)\n",
    "# Not in Contiguous US (NIC)\n",
    "\n",
    "color_dict_born = {}\n",
    "\n",
    "color_dict_born['ENG']=Category20b[20][1] # New England (purple)\n",
    "color_dict_born['MAT']=Category20b[20][3] # Mid Atlantic (light purple)\n",
    "color_dict_born['SAT']=Category20b[20][-2] # South Atlantic (pinkish)\n",
    "color_dict_born['ESC']=Category20b[20][-4] # East South Central (purplish-pink)\n",
    "color_dict_born['WSC']=Category20b[20][-6] # West South Central (rose-ish)\n",
    "color_dict_born['ENC']=Category20c[20][3] # East North Central (light blue)\n",
    "color_dict_born['WNC']=Category20c[20][0] # West North Central (blue)\n",
    "color_dict_born['MNT']=Category20b[20][5] # Mountain (Green)\n",
    "color_dict_born['PAC']=Category20b[20][7] # Pacific (lighter green)\n",
    "color_dict_born['NIC']=Category20c[20][-3] # Not in contiguous US (grey)\n",
    "\n",
    "color_dict_race_hisp = {}\n",
    "\n",
    "color_dict_race_hisp['B_H']=Category20b[20][2] # Black and Hispanic\n",
    "color_dict_race_hisp['B_N']=Category20b[20][3] # Black and not Hispanic\n",
    "color_dict_race_hisp['O_H']=Category20b[20][4] # Other and Hispanic\n",
    "color_dict_race_hisp['O_N']=Category20b[20][6] # Other and not Hispanic\n",
    "color_dict_race_hisp['W_H']=Category20c[4][-3] # White and Hispanic\n",
    "color_dict_race_hisp['W_N']=Category20c[4][-1] # White and not Hispanic\n",
    "\n",
    "color_dict_race_hisp_mex = {}\n",
    "\n",
    "color_dict_race_hisp_mex['B_H_O']=Category20c[5][-1]# Black Hispanic non-Mexican (orange)\n",
    "color_dict_race_hisp_mex['B_N_N']=Category20c[7][-1] # Black not Hispanic (light orange)\n",
    "color_dict_race_hisp_mex['O_H_M']=Category20b[20][4] # Other Hispanic Mexican (dark green)\n",
    "color_dict_race_hisp_mex['O_H_O']=Category20b[20][6] # Other Hispanic non-Mexican (green)\n",
    "color_dict_race_hisp_mex['O_N_N']=Category20c[13][-1] # Other not Hispanic non-Mexican (poiple)\n",
    "color_dict_race_hisp_mex['W_H_M']=Category20c[9][-1] # White Hispanic Mexican (green)\n",
    "color_dict_race_hisp_mex['W_H_O']=Category20c[11][-1] # White Hispanic non-Mexican (lighter green)\n",
    "color_dict_race_hisp_mex['W_H_U']=Category20c[12][-1] # White Hispanic Unknown (even lighter green)\n",
    "color_dict_race_hisp_mex['W_N_N']=Category20c[4][0] # White not Hispanic non-Mexican (blue!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set up an index of each population member and vice versa\n",
    "# We want to quickly access a given individual's population and a given population's individuals\n",
    "population_by_individual_hrs = defaultdict(int)\n",
    "individuals_by_population_hrs = defaultdict(list)\n",
    "\n",
    "for a in aux_to_use:\n",
    "    population_by_individual_hrs[a[0]] = a[1]\n",
    "    individuals_by_population_hrs[a[1]].append(a[0])\n",
    "    \n",
    "indices_of_population_members_hrs = defaultdict(list)\n",
    "\n",
    "for index, indiv in enumerate(individuals_hrs):\n",
    "    try:\n",
    "        indices_of_population_members_hrs[population_by_individual_hrs[indiv]].append(index)\n",
    "    except KeyError:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12454, 3)"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a colour dictionary based on individual admixture proportions.\n",
    "# We have three values - this works nicely with an RGB tuple.\n",
    "hrs_joined_sorted = hrs_joined.values.tolist()\n",
    "\n",
    "temp_list = [h[-3:] for h in hrs_joined_sorted]\n",
    "hrs_joined_sorted_array = np.array(temp_list)\n",
    "hrs_joined_sorted_array = (255*hrs_joined_sorted_array).astype(np.int64)\n",
    "hrs_joined_sorted_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "color_dict_admix = defaultdict(int)\n",
    "for i in range(0, len(hrs_joined_sorted)):\n",
    "    color_dict_admix[i] = '#%02x%02x%02x' % (hrs_joined_sorted_array[i][0],\n",
    "                                             hrs_joined_sorted_array[i][1],\n",
    "                                             hrs_joined_sorted_array[i][2])\n",
    "\n",
    "color_list_admix = list()\n",
    "for i in range(0, len(hrs_joined_sorted)):\n",
    "    color_list_admix.append('#%02x%02x%02x' % (hrs_joined_sorted_array[i][0],\n",
    "                                             hrs_joined_sorted_array[i][1],\n",
    "                                             hrs_joined_sorted_array[i][2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[51,\n",
       " 92,\n",
       " 101,\n",
       " 102,\n",
       " 106,\n",
       " 109,\n",
       " 110,\n",
       " 116,\n",
       " 162,\n",
       " 163,\n",
       " 170,\n",
       " 185,\n",
       " 193,\n",
       " 207,\n",
       " 216,\n",
       " 232,\n",
       " 259,\n",
       " 269,\n",
       " 287,\n",
       " 330,\n",
       " 355,\n",
       " 358,\n",
       " 366,\n",
       " 398,\n",
       " 414,\n",
       " 438,\n",
       " 453,\n",
       " 462,\n",
       " 466,\n",
       " 497,\n",
       " 539,\n",
       " 584,\n",
       " 598,\n",
       " 609,\n",
       " 614,\n",
       " 644,\n",
       " 649,\n",
       " 657,\n",
       " 673,\n",
       " 700,\n",
       " 712,\n",
       " 720,\n",
       " 775,\n",
       " 786,\n",
       " 821,\n",
       " 829,\n",
       " 835,\n",
       " 847,\n",
       " 870,\n",
       " 901,\n",
       " 920,\n",
       " 940,\n",
       " 956,\n",
       " 963,\n",
       " 968,\n",
       " 976,\n",
       " 989,\n",
       " 994,\n",
       " 1009,\n",
       " 1028,\n",
       " 1038,\n",
       " 1079,\n",
       " 1087,\n",
       " 1095,\n",
       " 1102,\n",
       " 1123,\n",
       " 1131,\n",
       " 1137,\n",
       " 1155,\n",
       " 1173,\n",
       " 1181,\n",
       " 1190]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(list(set(hrs_subset_indices) & set(indices_of_population_members_hrs['NIC'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate interactive HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a function to generate interactive HTML files\n",
    "def hrs_create_int_html(proj, fig_title, fname, page_title):\n",
    "    # Import TSNE PC projections from file and export HTML\n",
    "    temp_array = proj\n",
    "\n",
    "    component_1_id = 0\n",
    "    component_2_id = 1\n",
    "\n",
    "    p = figure(plot_width=1500, plot_height=800)\n",
    "    p.title.text = fig_title\n",
    "\n",
    "    for pop in sorted(eth_proxy_set):\n",
    "        proj_pop = temp_array[indices_of_population_members_hrs[pop]]\n",
    "        p.circle(proj_pop[:,component_1_id], proj_pop[:,component_2_id], legend=pop_dict[pop],\n",
    "                 color = color_dict[pop])\n",
    "\n",
    "    p.legend.location = \"top_left\"\n",
    "\n",
    "    p.legend.click_policy=\"hide\"\n",
    "\n",
    "    output_file(fname + '.html',title=page_title)\n",
    "\n",
    "    save(p)\n",
    "    print('Saved interactive HTML for ' + fig_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Same function as above, but coloured in by estimated admixture proportions\n",
    "def hrs_create_int_html_admix(proj, ptitle, fname, ftitle):\n",
    "    p = figure(plot_width=1500, plot_height=800)\n",
    "    p.title.text = ptitle\n",
    "\n",
    "    for pop in sorted(eth_proxy_set):\n",
    "        proj_pop = proj[indices_of_population_members_hrs[pop]]\n",
    "        p.circle(proj_pop[:,0], proj_pop[:,1], legend=pop_dict[pop],\n",
    "                 color = [color_list_admix[i] for i in indices_of_population_members_hrs[pop]])\n",
    "\n",
    "    p.legend.location = \"top_left\"\n",
    "\n",
    "    p.legend.click_policy=\"hide\"\n",
    "\n",
    "    output_file(fname + '.html',title=ftitle)\n",
    "\n",
    "    save(p)\n",
    "    print('Saved interactive admixture HTML for ' + fig_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hrs_create_image(proj, ptitle, fname, ftitle):\n",
    "    proj = np.loadtxt(proj)\n",
    "\n",
    "    fig = plt.figure(figsize=(20,20))\n",
    "    ax = fig.add_subplot(111, aspect=1)\n",
    "\n",
    "    for pop in sorted(eth_proxy_set):\n",
    "        temp_proj = proj[indices_of_population_members_hrs[pop]]\n",
    "        ax.scatter(temp_proj[:,0], temp_proj[:,1], label=pop_dict[pop], alpha=0.6, color=color_dict[pop])\n",
    "\n",
    "    ax.legend(ncol=3,loc='lower center', bbox_to_anchor=(0.55,-0.15), fontsize=12,markerscale=3)\n",
    "    fig.savefig(fname + '.jpeg',format='jpeg')\n",
    "    plt.close()\n",
    "    print('Saved image for ' + fig_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved interactive HTML for HRS_UMAP_PC10_NN15_MD0.5_2018330153123\n",
      "could not convert string to float: b'['\n",
      "HRS_UMAP_PC10_NN15_MD0.5_2018330153123\n"
     ]
    }
   ],
   "source": [
    "proj_dir = '/Volumes/Stockage/alex/hrs/projections'\n",
    "out_dir = '/Volumes/Stockage/alex/hrs/sandbox'\n",
    "\n",
    "file = 'HRS_UMAP_PC10_NN15_MD0.5_2018330153123'\n",
    "\n",
    "if aux_to_use == aux_data_1:\n",
    "    # Not used\n",
    "    aux_label = 'BORN_RACE_HISP_MEX'\n",
    "elif aux_to_use == aux_data_2:\n",
    "    aux_label = 'RACE_HISP_MEX'\n",
    "    color_dict = color_dict_race_hisp_mex\n",
    "elif aux_to_use == aux_data_3:\n",
    "    # Not used\n",
    "    aux_label = 'BORN_RACE'\n",
    "elif aux_to_use == aux_data_4:\n",
    "    aux_label = 'RACE_HISP'\n",
    "    color_dict = color_dict_race_hisp\n",
    "elif aux_to_use == aux_data_5:\n",
    "    aux_label = 'BORN'\n",
    "    color_dict = color_dict_born\n",
    "elif aux_to_use == aux_data_6:\n",
    "    # Not used\n",
    "    aux_label = 'BORN_HISP_MEX'\n",
    "\n",
    "if subset=='All':\n",
    "    try:\n",
    "        temp_proj = np.loadtxt(os.path.join(proj_dir, file))\n",
    "        out_fig_title = file\n",
    "        out_file = os.path.join(out_dir, file + '_' + aux_label)\n",
    "        out_page_title = file\n",
    "        hrs_create_int_html(temp_proj, out_fig_title, out_file, out_page_title)\n",
    "        hrs_create_image(temp_proj, out_fig_title, out_file, out_page_title)\n",
    "        hrs_create_int_html_admix(temp_proj, out_fig_title, out_file+'_ADMIX', out_page_title)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(file)\n",
    "elif subset=='Hispanic':\n",
    "    # Work only with the Hispanic subset\n",
    "    try:\n",
    "        temp_proj = np.loadtxt(os.path.join(proj_dir, file))\n",
    "        out_fig_title = file\n",
    "        out_file = os.path.join(out_dir, file + '_' + aux_label)\n",
    "        out_page_title = file\n",
    "        hrs_create_int_html(temp_proj, out_fig_title, out_file, out_page_title)\n",
    "        hrs_create_image(temp_proj, out_fig_title, out_file, out_page_title)\n",
    "        hrs_create_int_html_admix(temp_proj, out_fig_title, out_file+'_ADMIX', out_page_title)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(file)\n",
    "elif subset=='Black':\n",
    "    try:\n",
    "        temp_proj = np.loadtxt(os.path.join(proj_dir, file))\n",
    "        out_fig_title = file\n",
    "        out_file = os.path.join(out_dir, file + '_' + aux_label)\n",
    "        out_page_title = file\n",
    "        hrs_create_int_html(temp_proj, out_fig_title, out_file, out_page_title)\n",
    "        hrs_create_image(temp_proj, out_fig_title, out_file, out_page_title)\n",
    "        hrs_create_int_html_admix(temp_proj, out_fig_title, out_file+'_ADMIX', out_page_title)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(file)\n",
    "elif subset=='White':\n",
    "    try:\n",
    "        temp_proj = np.loadtxt(os.path.join(proj_dir, file))\n",
    "        out_fig_title = file\n",
    "        out_file = os.path.join(out_dir, file + '_' + aux_label)\n",
    "        out_page_title = file\n",
    "        hrs_create_int_html(temp_proj, out_fig_title, out_file, out_page_title)\n",
    "        hrs_create_image(temp_proj, out_fig_title, out_file, out_page_title)\n",
    "        hrs_create_int_html_admix(temp_proj, out_fig_title, out_file+'_ADMIX', out_page_title)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
